{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400cf36e-fd75-4aa5-ab3c-435b61d429f6",
   "metadata": {},
   "source": [
    "# Bayesian MBG estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e1a93-6dcf-4650-b132-0804b7ea15bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Geostatistical Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b76b10a-d4d2-46e3-82c9-33f84a74183d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "#import aesara.tensor as at\n",
    "#from aesara.graph.basic import Constant\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pytensor.tensor as at\n",
    "import functions\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e963da62-f8d4-45f8-9b99-27a5d861889d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 8\n"
     ]
    }
   ],
   "source": [
    "# Get the number of CPU cores to max out the machine in the traning stage\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "print(f\"Number of CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dfb7c0-e567-4932-a955-80bc250cc348",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd783437-e4a1-4e88-bcff-e3450d023b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load report\n",
    "with open('temp_files/report/report.pkl', 'rb') as pickle_file:\n",
    "    report = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a32c0d-bd75-4d8f-8e83-db97faf5c1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Country': 'KHM',\n",
       " 'DHS survey year': 2021,\n",
       " 'Target Indicator': 'mpi',\n",
       " 'Urban-rural threshold': 25,\n",
       " 'DHS clusters before urban masking': 709,\n",
       " 'Grids before masking': 7793,\n",
       " 'CRS': 'WGS 84',\n",
       " 'Grids after masking': 7667,\n",
       " 'DHS clusters after masking': 660,\n",
       " 'Grids with multiple clusters': 44,\n",
       " 'Covariates before filtering': 215,\n",
       " 'Covariates after filtering by -+ 1y': 66,\n",
       " 'Variogram Range': 0.13613857203004207,\n",
       " 'Suggested ls beta': 2.0,\n",
       " 'Covariates after square and square-root transforms': 61,\n",
       " 'Covariates after pairwise interactions': 1891,\n",
       " 'Target mean pre transform': 0.09821148197805704,\n",
       " 'Target std pre transform': 0.09589259726513627,\n",
       " 'Transformation Applied': 'log',\n",
       " 'Lambda': None,\n",
       " 'Number of covariates selected after Lasso': 19}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b7895e6-677b-4691-b151-d288cc4d333e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_indicator = report['Target Indicator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5c6412-da59-4b27-982a-ca71851c01ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the directory where the pickle files are stored\n",
    "pickle_dir = 'temp_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf4655-7c75-4007-a06e-f57d8d3c1106",
   "metadata": {},
   "source": [
    "### Load the target and the covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee20c5a-cbaa-447c-ada0-e48a2cfa59d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = pd.read_pickle('temp_files/selected_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09181019-9490-4cf4-ba8c-fbe4979d8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates from the geometry column (using centroid for Polygons)\n",
    "coordinates = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf.geometry])\n",
    "\n",
    "# Extract coordinates from the geometry column (using centroid for Polygons) only for observed rows\n",
    "coordinates_observed = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf[~gdf[target_indicator].isnull()].geometry])\n",
    "\n",
    "# Extract coordinates from the geometry column (using centroid for Polygons) only for unobserved rows\n",
    "coordinates_unobserved = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf[gdf[target_indicator].isnull()].geometry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d319008a-3927-4d40-83bf-8ee13c4f7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop geometry\n",
    "df = gdf.drop('geometry', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27ad7694-6275-464b-bbea-dddf156a2875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Only rows with observed target indicator\n",
    "df1 = df[~df[target_indicator].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c82ebcfa-93cf-454c-b61e-2c82c50785e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save sampled df1 for next notebook\n",
    "with open('temp_files/report/df1_sample.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(df1, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a6a622-e2f3-40e9-8f05-da370313c84f",
   "metadata": {},
   "source": [
    "### Transform the target indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "807d1253-a6e8-45a4-8444-d7805a68cf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness: 1.5221212479966317, Kurtosis: 2.8462013182848054\n",
      "High proportion of zeros detected (11.01%). Applying a shift of 0.01.\n",
      "Applying log transformation due to high positive skewness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wn/vzyjwy2x00q4stf523pr30jw0000gn/T/ipykernel_62904/538854860.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1[target_indicator] = target_transformed\n"
     ]
    }
   ],
   "source": [
    "## Select and apply the best transformation\n",
    "target_transformed, transform, lmda = functions.select_transformation(df1[[target_indicator]])\n",
    "\n",
    "## Store important information for reversion\n",
    "\n",
    "report['Transformation Applied'] = transform #Transformation applied to target\n",
    "report['Lambda'] = lmda #Store lambda variable for some reversion processes\n",
    "\n",
    "## Replace target for target_transformed in the df\n",
    "df1[target_indicator] = target_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d6189a-79f0-4873-a516-9146b12464df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = df1.columns.to_list()\n",
    "\n",
    "# Remove target_values and others\n",
    "remove_list = [target_indicator, 'geometry', 'grid_id']\n",
    "\n",
    "# Remove elements in remove_list from main_list\n",
    "selected_features = [item for item in selected_features if item not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61dee749-e44c-41bd-9673-7361cd837b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Covariate matrix\n",
    "X = df1[selected_features].values\n",
    "\n",
    "# Series with the target variable observed\n",
    "response = df1[target_indicator].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf5d988c-e248-4632-9c0d-a41be57a4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the mean and the std for inverse transformation\n",
    "report['Target mean'] = response.mean()\n",
    "report['Target std'] = response.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09dff1f2-65db-4dba-9a92-c11a7695d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95225ff3-20cb-425d-b022-a318447b4ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "response = scaler_y.fit_transform(response.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51698eae-69a6-4984-be01-62ab9d92868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarize the coordinates\n",
    "# Initialize the StandardScaler\n",
    "scaler_coordinates = StandardScaler()\n",
    "\n",
    "# Standardize the coordinates\n",
    "coordinates_observed = scaler_coordinates.fit_transform(coordinates_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4315db30-8ff4-41d8-8527-0aebef86577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['Observations used to train the model'] = response.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bee51-f03b-413a-bdc8-1d0450100073",
   "metadata": {},
   "source": [
    "### Bayesian Model Training\n",
    "\n",
    "Key Components of the Model\n",
    "\n",
    "    Priors:\n",
    "        beta: Coefficients for the linear model, assumed to follow a normal distribution with mean 0 and standard deviation 1.\n",
    "        sigma: Standard deviation of the observation noise, assumed to follow a half-normal distribution with standard deviation 1.\n",
    "        ls: Length-scale parameter for the spatial covariance function, assumed to follow a half-Cauchy distribution with scale parameter 1.\n",
    "\n",
    "    Spatial Distance Matrix:\n",
    "        D: Matrix of Euclidean distances between all pairs of observed locations.\n",
    "\n",
    "    Covariance Function:\n",
    "        K: Covariance function (Matern 5/2) which defines the spatial correlation structure.\n",
    "\n",
    "    Gaussian Process (GP):\n",
    "        gp: Latent Gaussian process with the defined covariance function.\n",
    "        f: Prior distribution of the GP evaluated at the observed coordinates.\n",
    "\n",
    "    Linear Model:\n",
    "        mu: Mean of the linear model which is a combination of the linear predictor (X * beta) and the spatial effect (f).\n",
    "        y_obs: Observed responses, modeled as a normal distribution with mean mu and standard deviation sigma.\n",
    "\n",
    "    Inference:\n",
    "        Using Automatic Differentiation Variational Inference (ADVI) to approximate the posterior distribution of the model parameters.\n",
    "        advi_fit: Fitting the model using ADVI.\n",
    "        trace: Sampling from the fitted model to obtain posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe474d3b-47bb-49ba-ba3d-2ffb956ca27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varibales to train on 445\n"
     ]
    }
   ],
   "source": [
    "print(f'Varibales to train on {response.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2e56dd6-72ec-47ee-94a1-4fba35ba0f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13613857203004207"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report['Variogram Range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa606405-d245-4987-a9e2-837650ae2bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors run\n",
      "Distance matrix calculated\n",
      "Covariance run\n",
      "Linear model specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (8 chains in 8 jobs)\n",
      "NUTS: [beta, sigma, ls, f_rotated_]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8720c65ae6394dfcb45b7b3e1e46a6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Fit a Bayesian geostatistical model\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    beta = pm.Normal('beta', mu=report['Target mean'], sigma=report['Target std'], shape=len(selected_features))\n",
    "    #beta = pm.Normal('beta', mu=0, sigma=1, shape=len(selected_features))\n",
    "    #sigma = pm.HalfNormal('sigma', sigma=report['Target std'])\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    \n",
    "   #ls = pm.HalfCauchy('ls', beta=report['Suggested ls beta'])\n",
    "\n",
    "    ls = pm.HalfCauchy('ls', beta=report['Variogram Range'])\n",
    "\n",
    "    print('Priors run')\n",
    "\n",
    "    # Spatial distance matrix\n",
    "    D = np.sqrt(((coordinates_observed[:, None, :] - coordinates_observed[None, :, :])**2).sum(axis=-1))\n",
    "\n",
    "    print('Distance matrix calculated')\n",
    "\n",
    "    # Define covariance function\n",
    "    K = pm.gp.cov.Matern52(2, ls=ls)\n",
    "    gp = pm.gp.Latent(cov_func=K)\n",
    "    f = gp.prior('f', X=coordinates_observed)\n",
    "\n",
    "    print('Covariance run')\n",
    "\n",
    "    # Linear model\n",
    "    ## This defines the mean of the normal distribution for the observed data. It combines a linear regression term (pm.math.dot(X, beta)) with the GP latent function f.\n",
    "    mu = pm.math.dot(X, beta) + f\n",
    "\n",
    "    ## This defines the likelihood of the observed data (response) as a normal distribution with mean mu and standard deviation sigma.\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=response)\n",
    "\n",
    "    print('Linear model specified')\n",
    "\n",
    "    # Inference\n",
    "    step = pm.NUTS(target_accept=0.95)\n",
    "    idata = pm.sample(1000, tune=1000, step=step, cores=num_cores, return_inferencedata=True) #The num_cores parameter maxes the machine out. Tweak if needed. \n",
    "\n",
    "    print('Model Fitted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f73a49-d761-45dd-8455-09767a68fa4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#az.to_netcdf(idata, trace_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b3166-cca2-409b-8bcc-ee678ac1673f",
   "metadata": {},
   "source": [
    "## Testing the model results\n",
    "\n",
    "1. Posterior Predictive Checks\n",
    "2. Prediction Accuracy Metrics\n",
    "3. Residual Analysis\n",
    "4. Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d960452-f2b8-480e-8571-17dd1e06afe0",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks:\n",
    "\n",
    "Posterior Predictive Distribution: Compare the observed data to the posterior predictive distribution of the model. This involves generating new data based on the posterior distributions of the model parameters and comparing these simulated data to the actual observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3707ef-231e-443a-9bb1-13368730d3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate posterior predictive samples for checks\n",
    "with model:\n",
    "    posterior_predictive = pm.sample_posterior_predictive(idata, var_names=['y_obs'], return_inferencedata=True)\n",
    "\n",
    "# Plot posterior predictive checks\n",
    "az.plot_ppc(posterior_predictive, kind='kde', data_pairs={'y_obs': 'y_obs'})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851b878-4927-4124-885f-d98fb9ac824b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "az.plot_forest(idata, var_names=[\"beta\"], combined=True, hdi_prob=0.95, r_hat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b1cd4b-d1f4-4867-955b-5df97334bf18",
   "metadata": {},
   "source": [
    "#### Trace plots\n",
    "\n",
    "Interpretation of Trace Plots\n",
    "\n",
    "    Density Plots (Left Column):\n",
    "        Each subplot on the left shows the kernel density estimate of the posterior distribution for a parameter.\n",
    "        These plots give an idea of the central tendency (mean or median) and the spread (variance) of the parameter estimates.\n",
    "        For example, the density plot for beta shows multiple colored curves corresponding to different chains, indicating the posterior distributions of the coefficients.\n",
    "\n",
    "    Trace Plots (Right Column):\n",
    "        Each subplot on the right shows the sampled values of the parameter across iterations for each chain.\n",
    "        These plots help in assessing the convergence of the Markov Chain Monte Carlo (MCMC) sampling.\n",
    "        A good trace plot should look like a \"hairy caterpillar,\" with the chains mixing well and no apparent trends or patterns over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6da08-c651-48d7-93c9-ca8410413560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the trace plot\n",
    "trace_plot = az.plot_trace(idata)\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('temp_files/report/9. trace_plot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101f5ba-a698-4133-a752-2605f5cf4f2b",
   "metadata": {},
   "source": [
    "### Residual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d93af3-4191-48dc-9930-bf49dd2637a7",
   "metadata": {},
   "source": [
    "#### Spatial Residual Plots \n",
    "\n",
    "Plot residuals (the differences between observed and predicted values) over the spatial domain to check for patterns. Randomly distributed residuals indicate a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1adc2-cb59-43bf-9700-1ad7de8bd4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the observed data\n",
    "y_obs = posterior_predictive.observed_data['y_obs'].values\n",
    "\n",
    "# Calculate the mean of the simulated data across chains and draws\n",
    "y_sim = posterior_predictive.posterior_predictive['y_obs'].mean(dim=('chain', 'draw')).values\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_obs - y_sim\n",
    "\n",
    "# Plot spatial residuals\n",
    "plt.figure(figsize=(12, 8))\n",
    "sc = plt.scatter(coordinates_observed[:, 0], coordinates_observed[:, 1], c=residuals, cmap='coolwarm', s=100)\n",
    "plt.colorbar(sc, label='Residuals')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Spatial Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82876d73-6a69-4288-af0a-7adc94733b35",
   "metadata": {},
   "source": [
    "### Uncertainty Quantification\n",
    "\n",
    "\t•\tCredible Intervals: Evaluate the width of the credible intervals for predictions. Narrower intervals indicate higher precision, but they should still encompass the true values.\n",
    "\t•\tUncertainty Maps: Generate maps of prediction uncertainty to visualize areas of high and low certainty in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499dd7bc-7f8d-4a21-a3d0-a98b000d7cf8",
   "metadata": {},
   "source": [
    "#### Credible Intervals\n",
    "\n",
    "Evaluate the width of the credible intervals for predictions. Narrower intervals indicate higher precision, but they should still encompass the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457e3cf-0b27-4744-995e-0a991fe1d6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the observed and simulated data\n",
    "y_obs = response\n",
    "\n",
    "# Generate posterior predictive samples for checks\n",
    "with model:\n",
    "    posterior_predictive = pm.sample_posterior_predictive(idata, var_names=['y_obs'], return_inferencedata=True)\n",
    "\n",
    "# Extract the mean prediction, lower and upper bounds of the 95% credible intervals\n",
    "y_sim = posterior_predictive.posterior_predictive['y_obs'].mean(dim=(\"chain\", \"draw\")).values\n",
    "hdi = az.hdi(posterior_predictive.posterior_predictive, hdi_prob=0.95)['y_obs']\n",
    "\n",
    "# Calculate the width of the credible intervals\n",
    "ci_width = hdi[:, 1] - hdi[:, 0]\n",
    "\n",
    "# Check how many true values are within the credible intervals\n",
    "within_ci = np.sum((y_obs >= hdi[:, 0]) & (y_obs <= hdi[:, 1]))\n",
    "total_obs = len(y_obs)\n",
    "coverage = within_ci / total_obs\n",
    "\n",
    "print(f\"Coverage of 95% Credible Intervals: {coverage * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot the width of the credible intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(ci_width, bins=30, edgecolor='k', alpha=0.5)\n",
    "plt.xlabel('Width of 95% Credible Intervals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the Width of 95% Credible Intervals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e247e-262f-4848-ba4b-b0e4faac3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the credible intervals vs the observed values\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.errorbar(np.arange(len(y_obs)), y_sim, yerr=[y_sim - hdi[:, 0], hdi[:, 1] - y_sim], fmt='o', alpha=0.7, label='Predictions with 95% CI')\n",
    "plt.plot(np.arange(len(y_obs)), y_obs, 'r.', label='Observed Values')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Predictions with 95% Credible Intervals vs Observed Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757b68f-dde4-4c13-af21-553bbf65e211",
   "metadata": {},
   "source": [
    "### Transform the results back to original scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b01b7-7230-497b-ba1c-67884369f483",
   "metadata": {},
   "source": [
    "#### Revert observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3316f3-7462-4bef-9a2a-4fb266ed43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_denormalized = functions.revert_standardization(response, report['Target mean'], report['Target std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fde956-1231-4817-8815-9943d34b44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_original_scale = functions.revert_transformation(y_denormalized, \n",
    "                                report['Transformation Applied'],\n",
    "                                report['Target mean'], \n",
    "                                report['Target std'], \n",
    "                               report['Lambda']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f3087-4ef5-487c-acb2-cff8cbeb25e0",
   "metadata": {},
   "source": [
    "#### Revert predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30acbab5-a3fe-4408-9f8f-181c272be5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sim_dn = functions.revert_standardization(y_sim, report['Target mean'], report['Target std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69f01f-8c6c-42bb-a67c-4e0d06bac966",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sim_original_scale = functions.revert_transformation(y_sim_dn, \n",
    "                                report['Transformation Applied'],\n",
    "                                report['Target mean'], \n",
    "                                report['Target std'], \n",
    "                               report['Lambda']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31e396-6a10-4766-8b4c-9bd934c6ed96",
   "metadata": {},
   "source": [
    "### Prediction Accuracy Metrics (at original scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e09b40-8184-48ac-b941-7bab86e785ca",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "Measures the average magnitude of the errors in a set of predictions, without considering their direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ebbcd9-d42f-418a-988d-07097e2db1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute errors\n",
    "absolute_errors = np.abs(y_original_scale - y_sim_original_scale)\n",
    "\n",
    "# Calculate the Mean Absolute Error\n",
    "mae = np.mean(absolute_errors)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3b8a7-31b3-4388-aa74-d0dc1ed9f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['Bayesian model MAE'] = mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45621bab-87c4-4f2f-9eaa-2e06b8f995a0",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error (RMSE) \n",
    "\n",
    "Measures the square root of the average of squared differences between predicted and observed values, providing an indication of the model’s overall error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80301fa7-627b-48dd-8501-6f5a24a7c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the squared errors\n",
    "squared_errors = np.square(y_original_scale - y_sim_original_scale)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE)\n",
    "mse = np.mean(squared_errors)\n",
    "\n",
    "# Calculate the Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60126662-399d-4453-86be-254279f8a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['Bayesian RMSE'] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68477e3a-d926-4b5c-9048-75bbb9112e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save report as pickle\n",
    "with open('temp_files/report/report.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(report, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbc5d4-5af5-44bb-8c46-baabb2fead42",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_report_keys = ['Variogram Range',\n",
    "                        'Transformation Applied', \n",
    "                        'Number of covariates selected after Lasso',  \n",
    "                        'Observations used to train the model',\n",
    "                        'Target mean', \n",
    "                        'Target std',\n",
    "                        'Target mean pre transform',\n",
    "                        'Target std pre transform',\n",
    "                        'Bayesian model MAE', \n",
    "                        'Bayesian RMSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd46b214-2d27-4df1-80e4-4f18cf449469",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_report = {key: report[key] for key in bayesian_report_keys if key in report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07ea01-ecab-4efc-8ad4-bc8fdb77130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.DataFrame(data={'Data':bayesian_report.keys(), \n",
    "                  'Value': bayesian_report.values()}, \n",
    "             columns=['Data','Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca2b79-9e91-4d8b-9f97-b27932fe3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = os.path.join('temp_files/report/', '14. Bayesian Report.pdf')\n",
    "title = 'Bayesian Model Report'\n",
    "functions.df_to_pdf(t1, table_path, title=title, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf60e18-df79-4e09-bf21-7feaf41b6ebd",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f213f-8414-4f03-8be9-2afee28fdbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = pd.read_pickle('temp_files/selected_features.pkl')\n",
    "gdf_full = pd.read_pickle('temp_files/selected_features_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1c0f3-3d32-47b2-81f8-02c388f71d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = gdf.columns.to_list()\n",
    "\n",
    "# Remove target_values and others\n",
    "remove_list = [target_indicator, 'geometry', 'grid_id']\n",
    "\n",
    "# Remove elements in remove_list from main_list\n",
    "selected_features = [item for item in selected_features if item not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Extract coordinates from the geometry column (using centroid for Polygons)\n",
    "coordinates = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf.geometry])\n",
    "\n",
    "#Standardize\n",
    "coordinates = scaler.fit_transform(coordinates)\n",
    "\n",
    "# Extract coordinates from the geometry column (using centroid for Polygons) only for observed rows\n",
    "coordinates_observed = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf[~gdf[target_indicator].isnull()].geometry])\n",
    "\n",
    "# Standardize\n",
    "coordinates_observed = scaler.fit_transform(coordinates_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129a75e-3878-47ef-8df9-7d83c3cfd474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Only rows with observed target indicator\n",
    "df1 = gdf[~gdf[target_indicator].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059c9ad-3b9c-400c-b02d-96175ffced8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select and apply the best transformation\n",
    "target_transformed, transform, lmda = functions.select_transformation(df1[[target_indicator]])\n",
    "\n",
    "#Store important information for reversion\n",
    "\n",
    "report['Transformation Applied'] = transform #Transformation applied to target\n",
    "report['Lambda'] = lmda #Store lambda variable for some reversion processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe8986-8b1e-4c74-be46-5243f0988069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace target for target_transformed in the df\n",
    "df1[target_indicator] = target_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d735d4c-eef8-4102-8a97-83f491d050ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformed target variable\n",
    "y = df1[target_indicator].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70c106-0aab-4bb8-9166-d16cf1efff9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Covariate matrix\n",
    "X = df1[selected_features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be0385-869b-4dc9-9bb0-400007dbd2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize features and transformed y\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "X = scaler_x.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3771651-e91f-4e1c-bbf8-2d33b3d7d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaler to a pickle file\n",
    "with open('scaler_y.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dad7f3-060d-42ca-b273-260046939a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF with all observations\n",
    "gdf2 = gdf_full.copy(deep=True) #This line to make predictions for all observations\n",
    "#gdf2 = gdf2.sample(1000)\n",
    "df2 = gdf2.copy(deep=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Extract coordinates from the geometry column (using centroid for Pcolygons)\n",
    "coordinates_new = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf2.geometry])\n",
    "\n",
    "# Standardize\n",
    "coordinates_new = scaler.fit_transform(coordinates_new)\n",
    "\n",
    "# Covariate matrix\n",
    "X_new = df2[selected_features].values\n",
    "\n",
    "# Standardize the new data using the same scaler fitted on the observed data\n",
    "X_new = scaler_x.transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852f05e-9c4e-4acd-9eb5-a094f3544f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_from_scratch = True\n",
    "    \n",
    "if begin_from_scratch == True: \n",
    "    #Dataframe to store predictions and other values for uncertainty calculations\n",
    "    df3 = pd.DataFrame(df2[['grid_id', 'mpi']].head(0))\n",
    "    start_position=0\n",
    "\n",
    "else:\n",
    "    #Pick-up productions were we left of. \n",
    "    with open('temp_files/predictions.pkl', 'rb') as pickle_file:\n",
    "        df3 = pickle.load(pickle_file)\n",
    "    start_position = df3.index.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776547d-c24e-469b-94f3-3d69e5fb963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10396f41-a434-4b0a-b174-4b69ebd9162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 95% confidence level, change alpha for different confidence levels\n",
    "alpha = 0.05\n",
    "z_score = stats.norm.ppf(1 - alpha/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e503963-25e4-4c67-ac9b-900d82842177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step = 434\n",
    "step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a50ad-17f1-4e46-9007-c7b82cae85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce69636-4ed9-4fac-baee-4d5ecfc60282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step is the number of observations to select in each iteration\n",
    "num_observations = len(X_new)\n",
    "all_indices = np.arange(num_observations)\n",
    "\n",
    "for i in range(start_position, num_observations, step):\n",
    "    \n",
    "    # Randomly select 'step' indices without replacement for each iteration\n",
    "    if step <= len(all_indices):\n",
    "        r = np.random.choice(all_indices, size=step, replace=False)\n",
    "        all_indices = np.setdiff1d(all_indices, r)  # Remove the selected indices from the pool\n",
    "    else:\n",
    "        # If fewer than 'step' indices remain, select all remaining indices\n",
    "        r = all_indices\n",
    "        all_indices = np.array([])\n",
    "\n",
    "    print(f\"Remaining indices: {len(all_indices)}\")\n",
    "\n",
    "    # Select the subset of new data points for this batch\n",
    "    X_new_i = X_new[r]\n",
    "    coordinates_new_i = coordinates_new[r]\n",
    "\n",
    "    # Generate a unique variable name for each iteration\n",
    "    f_pred_name = \"f_pred_\" + str(uuid.uuid4())\n",
    "\n",
    "    # Prediction generation using the Gaussian Process and posterior sampling\n",
    "    with model:\n",
    "        # Define the Gaussian Process covariance function for new points with jitter for numerical stability\n",
    "        f_pred = gp.conditional(f_pred_name, Xnew=coordinates_new_i, jitter=1e-2)\n",
    "    \n",
    "        # Sampling from the posterior predictive distribution for the new data points\n",
    "        ppc = pm.sample_posterior_predictive(idata, var_names=[f_pred_name], return_inferencedata=True)\n",
    "\n",
    "    # Extract 'beta' samples from the original posterior (idata)\n",
    "    beta_samples = idata.posterior['beta'].values  # Posterior samples for 'beta'\n",
    "\n",
    "    # Extract GP samples from posterior predictive\n",
    "    gp_samples = ppc.posterior_predictive[f_pred_name].values  # GP predictions\n",
    "\n",
    "    # Check dimensions\n",
    "    print(f\"beta_samples shape: {beta_samples.shape}\")  # Should be (num_samples, num_features)\n",
    "    print(f\"X_new_i shape: {X_new_i.shape}\")            # Should be (num_points, num_features)\n",
    "\n",
    "    # Calculate the linear component (dot product) for each sample\n",
    "    # Make sure the dimensions match: (num_samples, num_features) @ (num_features, num_points)\n",
    "    linear_component = np.dot(beta_samples, X_new_i.T)  # Shape: (num_samples, num_points)\n",
    "\n",
    "    # Combine the GP predictions and linear component\n",
    "    combined_predictions = gp_samples + linear_component\n",
    "\n",
    "    # Create DataFrame for all posterior samples (1000 samples for each observation)\n",
    "    dfpi = pd.DataFrame(combined_predictions.mean(axis=0))  # The shape should be (1000, num_points)\n",
    "\n",
    "    # Extracting relevant data for the current batch\n",
    "    dfi = df2.loc[r][['grid_id', target_indicator]]\n",
    "\n",
    "    # Combine predictions with the data\n",
    "    dfi[target_indicator] = dfpi.mean(axis=0).values  # Mean prediction across samples\n",
    "    dfi['std'] = dfpi.std(axis=0).values               # Standard deviation across samples\n",
    "    dfi['max'] = dfpi.max(axis=0).values               # Max prediction across samples\n",
    "    dfi['min'] = dfpi.min(axis=0).values               # Min prediction across samples\n",
    "\n",
    "    # Calculating the margin of error for the confidence interval\n",
    "    dfi['standard_error'] = dfi['std'] / (len(dfpi)**0.5)\n",
    "    dfi['margin_of_error'] = z_score * dfi['standard_error']\n",
    "\n",
    "    # Calculating the confidence interval\n",
    "    dfi['ci_lower'] = dfi[target_indicator] - dfi['margin_of_error']\n",
    "    dfi['ci_upper'] = dfi[target_indicator] + dfi['margin_of_error']\n",
    "\n",
    "    # Combine with previous results\n",
    "    df3 = pd.concat([df3, dfi])\n",
    "\n",
    "    # Save predictions so far\n",
    "    dfpi.to_pickle('temp_files/dfpi.pkl')  # Save all predictions for the current batch\n",
    "    df3.to_pickle('temp_files/predictions.pkl')\n",
    "\n",
    "    # Break the loop if no more indices are left to process\n",
    "    if len(all_indices) == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bea840-7eb7-44cf-9636-e26a748ca398",
   "metadata": {},
   "source": [
    "## Covariate Coefficients Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626900f0-d9cb-425e-a7ef-7a9f3830b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the posterior distribution for beta\n",
    "beta_posterior = idata.posterior['beta']\n",
    "\n",
    "# Compute the mean and 94% highest posterior density (HPD) intervals\n",
    "beta_mean = beta_posterior.mean(dim=[\"chain\", \"draw\"])\n",
    "beta_hpd = pm.hdi(beta_posterior, hdi_prob=0.94)\n",
    "\n",
    "# Display the results\n",
    "print(\"Covariate Coefficients (Beta):\")\n",
    "print(\"Posterior Mean:\\n\", beta_mean)\n",
    "print(\"94% HPD Interval:\\n\", beta_hpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7327abe-3e33-4e14-a4f0-3d9b62d56738",
   "metadata": {},
   "outputs": [],
   "source": [
    "t9 = pd.DataFrame([selected_features, beta_mean.values]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1f179-0079-4a43-b0fb-29c7adf6b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t9.columns = ['Covariate', 'Coefficient (standardized)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7422d-5622-4ecd-a005-bc5ee31fd8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t9['Coefficient (standardized)'] = t9['Coefficient (standardized)'].astype(float).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f72a3-8f7d-4521-94e3-32ba1f4e0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = os.path.join('temp_files/report/', '15. Bayesian Coefficients.pdf')\n",
    "title = 'Bayesian Model Coefficients'\n",
    "functions.df_to_pdf(t9, table_path, title=title, show=True)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "pymc_env",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
